import torch
import torchaudio
import whisper
import numpy as np
import librosa
from pyannote.audio import Pipeline
from pyannote.core import Annotation

# --- AYARLAR ---
AUDIO_FILE = "C:\\Users\\baris\\Downloads\\kayit.wav"
HF_TOKEN = "xxxxxhf_ehtqrATTRJXAWEXZGmdFfAykWUerakESHZ"
WHISPER_MODEL_NAME = "medium"
# ----------------

def diarization_and_transcription(audio_file_path, hf_token, whisper_model_name):
    """
    CUDA destekli konuÅŸmacÄ± ayÄ±rma + transkripsiyon
    """
    # --- CUDA kontrolÃ¼ ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âš™ï¸ KullanÄ±lan cihaz: {device}")

    # --- PYANNOTE DÄ°ARIZATION ---
    print("1ï¸âƒ£ pyannote.audio ile konuÅŸmacÄ± bÃ¶lÃ¼mlendirme (Diarization) baÅŸlatÄ±lÄ±yor...")

    try:
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=hf_token
        )
        pipeline.to(device)  # GPU'ya taÅŸÄ±
    except Exception as e:
        print(f"âŒ Hata: pyannote modelini yÃ¼klerken sorun oluÅŸtu.\nAÃ§Ä±klama: {e}")
        return

    # --- SESÄ° YÃœKLEME ---
    print("ğŸ§ Sesi torchaudio ile yÃ¼klÃ¼yoruz...")
    try:
        waveform, sample_rate = torchaudio.load(audio_file_path)
        waveform = waveform.to(torch.float32).to(device)
        pyannote_audio = {"waveform": waveform.cpu(), "sample_rate": int(sample_rate)}
    except Exception as e:
        print(f"âŒ Hata: torchaudio.load baÅŸarÄ±sÄ±z oldu.\nAÃ§Ä±klama: {e}")
        return

    # --- DÄ°ARIZATION ---
    print("ğŸ” KonuÅŸmacÄ± ayÄ±rma iÅŸlemi baÅŸlÄ±yor...")
    try:
        diarization = pipeline(pyannote_audio)
    except Exception as e:
        print(f"âŒ Hata: Diarization sÄ±rasÄ±nda sorun oluÅŸtu.\nAÃ§Ä±klama: {e}")
        return

    diarization_segments = []
    annotation = diarization.speaker_diarization if hasattr(diarization, "speaker_diarization") else diarization
    for segment, _, speaker in annotation.itertracks(yield_label=True):
        diarization_segments.append({
            "start": float(segment.start),
            "end": float(segment.end),
            "speaker": speaker
        })

    print(f"âœ… Diarization tamamlandÄ±. {len(diarization_segments)} segment bulundu.")

    # --- WHISPER MODELÄ° ---
    print("2ï¸âƒ£ Whisper modeli yÃ¼kleniyor...")
    try:
        model = whisper.load_model(whisper_model_name, device=str(device))
    except Exception as e:
        print(f"âŒ Hata: Whisper modeli yÃ¼klenemedi.\nAÃ§Ä±klama: {e}")
        return

    # --- TRANSKRÄ°PSÄ°YON ---
    print("ğŸ“ Transkripsiyon baÅŸlatÄ±lÄ±yor...")
    try:
        audio, sr = librosa.load(audio_file_path, sr=16000, mono=True)
    except Exception as e:
        print(f"âŒ Hata: Ses dosyasÄ±nÄ± librosa ile yÃ¼klerken sorun oluÅŸtu.\nAÃ§Ä±klama: {e}")
        return

    sr = 16000
    final_transcript = []

    for seg in diarization_segments:
        start_sec = seg["start"]
        end_sec = seg["end"]
        speaker = seg["speaker"]

        start_sample = int(start_sec * sr)
        end_sample = int(end_sec * sr)

        segment_audio = audio[start_sample:end_sample].astype(np.float32)
        if segment_audio.size == 0:
            continue

        # Segmenti GPU'ya aktar
        segment_tensor = torch.from_numpy(segment_audio).to(device)

        result = model.transcribe(segment_tensor, language="tr")
        text = result["text"].strip()

        if text:
            final_transcript.append({
                "start": start_sec,
                "end": end_sec,
                "speaker": speaker,
                "text": text
            })
            print(f"[{start_sec:.2f}s - {end_sec:.2f}s] {speaker}: {text}")

    # --- SONUÃ‡ ---
    print("\n--- ğŸ§¾ Nihai Transkript ---")
    for item in final_transcript:
        print(f"{item['speaker']} ({item['start']:.2f}s - {item['end']:.2f}s): {item['text']}")

if __name__ == "__main__":
    diarization_and_transcription(AUDIO_FILE, HF_TOKEN, WHISPER_MODEL_NAME)
